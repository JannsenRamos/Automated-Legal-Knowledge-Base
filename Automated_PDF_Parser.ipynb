{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3MjcRti/Ivg8u8C9fd8rJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JannsenRamos/Automated-Legal-Knowledge-Base/blob/main/Automated_PDF_Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic langchain_openai pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr9P4oMmwznZ",
        "outputId": "616f0c61-83fb-43f0-b384-cba0e0d8bc76"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.2.6)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.14.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.13.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain_openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf, langchain_openai\n",
            "Successfully installed langchain_openai-1.1.7 pymupdf-1.26.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d-UmcB5avkwE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from typing import List, Optional, Union, Literal\n",
        "from datetime import datetime\n",
        "from google.colab import drive, userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SETUP & IMPORTS\n",
        "from pydantic import BaseModel, Field, ConfigDict\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "#MOUNT DRIVE\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/My Drive/Labor_Law_System\"\n",
        "JSON_DIR = os.path.join(BASE_DIR, \"structured_json\")\n",
        "os.makedirs(JSON_DIR, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmmHHk0LvrHZ",
        "outputId": "d5d51477-0e00-4a43-ed3e-5840b52e3266"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined the Pydantic Template\n",
        "class DocumentMetadata(BaseModel):\n",
        "    source_file: str\n",
        "    file_type: str\n",
        "    page_number: int\n",
        "    corpus_category: str  # e.g., \"wages\", \"contracts\"\n",
        "    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())\n",
        "\n",
        "class LaborArticleChunk(BaseModel):\n",
        "    node_type: Literal[\"labor_article\"] = \"labor_article\"\n",
        "    article_number: int\n",
        "    old_article_number: Optional[int] = None\n",
        "    title: str\n",
        "    content: str\n",
        "    is_repealed: bool = False\n",
        "    metadata: DocumentMetadata\n",
        "\n",
        "class DocumentManifest(BaseModel):\n",
        "    \"\"\"The Root structure for structured output.\"\"\"\n",
        "    project_name: str = \"Labor Law Knowledge Base\"\n",
        "    chunks: List[LaborArticleChunk]"
      ],
      "metadata": {
        "id": "tXJZbc4ivrgL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Router to check which template fits the pdf file\n",
        "llm_router = ChatOpenAI(\n",
        "    model=\"openai/gpt-oss-120b:free\",\n",
        "    api_key=userdata.get('OPENROUTER_API_KEY'),\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "def identify_document_pattern(sample_text):\n",
        "    prompt = f\"Identify the type of this legal text. Return 'LABOR_CODE' or 'GENERIC'. Text: {sample_text[:500]}\"\n",
        "    try:\n",
        "        response = llm_router.invoke(prompt)\n",
        "        return response.content.strip()\n",
        "    except:\n",
        "        return \"LABOR_CODE\" # Default fallback"
      ],
      "metadata": {
        "id": "67JLO3vdvsAj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applies Selective Indexing using specific keywords for each chunks for separation\n",
        "ROUTING_RULES = {\n",
        "    \"wages\": [\"wage\", \"salary\", \"pay\", \"overtime\", \"payroll\", \"deduction\", \"night shift\"],\n",
        "    \"contracts\": [\"contract\", \"dismissal\", \"tenure\", \"termination\", \"probationary\", \"resignation\"],\n",
        "}\n",
        "\n",
        "def local_production_extractor(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    file_name = os.path.basename(pdf_path)\n",
        "\n",
        "    # Regex: Finds \"ART. 130 [132]\"\n",
        "    pattern = r\"ART\\.\\s+(\\d+)(?:\\s+\\[(\\d+)\\])?\"\n",
        "\n",
        "    full_text = \"\".join([page.get_text() for page in doc])\n",
        "    parts = re.split(pattern, full_text)\n",
        "\n",
        "    processed_count = 0\n",
        "    # Parts array is: [prefix, art_num, old_num, content, ...]\n",
        "    for i in range(1, len(parts), 3):\n",
        "        art_num = parts[i]\n",
        "        old_num = parts[i+1]\n",
        "        content = parts[i+2].strip()\n",
        "\n",
        "        # Keyword-based Routing\n",
        "        category = \"general\"\n",
        "        for cat, keywords in ROUTING_RULES.items():\n",
        "            if any(k in content.lower() for k in keywords):\n",
        "                category = cat; break\n",
        "\n",
        "        # Build Metadata\n",
        "        meta = DocumentMetadata(\n",
        "            source_file=file_name,\n",
        "            file_type=\"legal_code\",\n",
        "            page_number=0, # Simplified for full-text split\n",
        "            corpus_category=category\n",
        "        )\n",
        "\n",
        "        # Create Pydantic Object\n",
        "        chunk = LaborArticleChunk(\n",
        "            article_number=int(art_num),\n",
        "            old_article_number=int(old_num) if old_num else None,\n",
        "            title=content.split('\\n')[0][:100], # First line as title\n",
        "            content=content,\n",
        "            is_repealed=\"repealed\" in content.lower() or \"ra 10151\" in content.lower(),\n",
        "            metadata=meta\n",
        "        )\n",
        "\n",
        "        # Save to Category Folder\n",
        "        save_path = os.path.join(JSON_DIR, category)\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        with open(f\"{save_path}/art_{art_num}.json\", \"w\") as f:\n",
        "            f.write(chunk.model_dump_json(indent=2))\n",
        "\n",
        "        processed_count += 1\n",
        "\n",
        "    print(f\"Finished! Processed {processed_count} articles into {JSON_DIR}\")\n",
        "\n",
        "# --- 5. EXECUTION ---\n",
        "pdf_to_process = \"/content/2022.Labor_.Code_.DOLE-edition.pdf\"\n",
        "local_production_extractor(pdf_to_process)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89SFUTONxhWu",
        "outputId": "9b8d081d-4feb-499e-d65d-4fb40c902df0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished! Processed 319 articles into /content/drive/My Drive/Labor_Law_System/structured_json\n"
          ]
        }
      ]
    }
  ]
}